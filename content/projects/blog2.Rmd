---
categories:
- ""
- ""
date: "2017-10-31T22:42:51-05:00"
description: Nullam et orci eu lorem consequat tincidunt vivamus et sagittis magna
  sed nunc rhoncus condimentum sem. In efficitur ligula tate urna. Maecenas massa
  sed magna lacinia magna pellentesque lorem ipsum dolor. Nullam et orci eu lorem
  consequat tincidunt. Vivamus et sagittis tempus.
draft: false
image: pic07.jpg
keywords: ""
slug: aliquam
title: Aliquam
---

```{r, setup, echo=FALSE}
knitr::opts_chunk$set(
  message = FALSE, 
  warning = FALSE, 
  tidy=FALSE,     # display code as typed
  size="small")   # slightly smaller font for code
options(digits = 3)

# default figure size
knitr::opts_chunk$set(
  fig.width=6.75, 
  fig.height=6.75,
  fig.align = "center"
)
```


```{r load-libraries, echo=FALSE}
library(tidyverse)  # Load ggplot2, dplyr, and all the other tidyverse packages
library(mosaic)
library(ggthemes)
library(GGally)
library(readxl)
library(here)
library(skimr)
library(janitor)
library(broom)
library(tidyquant)
library(infer)
library(openintro)
```


# Youth Risk Behavior Surveillance

To begin with we will look into a survey of health and activity measures. 

We will use the [Youth Risk Behavior Surveillance System (YRBSS)](https://www.cdc.gov/healthyyouth/data/yrbs/index.htm) survey, where it takes data from high schoolers (9th through 12th grade). 

This will be useful as it provides a comprehensive overview of things such as weight, height, physical activity and more.

## Load the data

First we want to load the data. We do that below: 

```{r}

#Loading Data from the openintro. 
data(yrbss)

#Getting a brief overview of the data
glimpse(yrbss)

#Summarizing the data
skim(yrbss)

```



From the above we can see the data. We note that some values are missing in the dataset which we will keep in mind when going forward. 


## Exploratory Data Analysis

We would like to explore this data further. We will first start with analyzing the `weight` of participants in kilograms.

```{r, eda_on_weight}

#Choosing the dataset to work with
yrbss_weight <- yrbss %>%
  
  #Filtering for weight only
  select(weight)

#Looking at summary statistics and skim function allows us to understand the data, 
#as well as seeing whether there are missing observations 
summary(yrbss_weight)
skim(yrbss_weight)

#We are missing 1,004 observations in this dataset which we will keep in mind going forward. 

#PLotting the density of the weights
ggplot(yrbss_weight, aes(x=weight))+
  
  #density plot with transparancy set to 20% and in color blue
  geom_density(alpha=0.2,fill="blue") +   
  
  #Simple theme
  theme_bw() + 
  
  #Adding Useful titles
  labs (
    title = "Analysis of Participants' Weights using density plot method",
    x     = "Participants' Weight",
    y = "Density" #changing y-axis label to sentence case, 
  ) + 
  
  NULL


```
With the skim and summary functions, we notice that 1004 observations concerning participants' weight are missing. Additionally, the distribution of the weighs seems to follow a normal distribution with data points almost equally spread around the mean. The curve is skewed to the right (positively skewed), meaning weights above the sample's mean reach numbers that are further apart from the mean compared to weights below it.


Next, we want to consider the possible relationship between a high schoolerâ€™s weight and their physical activity. Plotting the data is a useful first step because it helps us quickly visualize trends, identify strong associations, and develop research questions.

First we will create an overview of the amount of people that are active three days or more versus those who are not. That is computed below:

  
```{r, mutate_and_count}

# Setting up a new variable and removing N/A variables
yrbss_1<-yrbss %>%
  
  #Removing N/A Varialbes
  na.omit(physically_active_7d) %>%
  na.omit(weight)



#Below section calculates how many are active 3 days or more
physical_3plus<-yrbss_1 %>% 
  
  #Active days above or equal to 3 defined as active
  filter(physically_active_7d>=3) %>% 
  
  #Summarizing by number of active days
  summarize(Active=n(), #Count of observations with three days or more
            Inactive=nrow(yrbss_1)-n(), #Same for inactive
            Active_prct=Active*100/nrow(yrbss_1), 
            Inactive_prct=(100-Active_prct)) 

#We print the results
physical_3plus

#We see that most people (5,695 out of 8,351) are active 3 days or more a week. 

#We compute an additional overview below to see if we get the same results as when we use group by. 

#We added a new variable which will be used to test if people are physically active
physical_3plus_1<-yrbss_1 %>%
  
  #Adding whether or not people are physically active by yes or no
  mutate(physical_3plus=ifelse(physically_active_7d>=3, "yes","no")) %>% 

  #Grouping by active days
  group_by(physical_3plus) %>% 
  
  #Summarizing the data to count active/inactive days
  summarize(Count=n(),
            Percentage=Count*100/nrow(yrbss_1))
  
#Printing the group by methodology
physical_3plus_1

```


Within our new dataframe, physical_3plus, people are allocated a "yes" if they are physically active for at least 3 days while they are allocated a "no" otherwise. We also computed the number and % of both categories.

Also we get the same results from using n() compared to group by.

We see that most people are active for three days r more (68.2%) 

We want to provide a 95% confidence interval for the population proportion of high schools that are *NOT* active 3 or more days per week:
  
In the following section we will make a boxplot of `physical_3plus` vs. `weight`, to look at the relationship between the two variables.

```{r, boxplot}

#adding a new variable which will be used to test if people are physically active
physical_3plus<-yrbss_1 %>%
  mutate(physical_3plus=ifelse(physically_active_7d>=3, "yes","no"))

#plotting the data for relationship between weight and physical activeness
ggplot(physical_3plus,aes(x=weight,y=physical_3plus))+
  
  #Using Boxlplot
  geom_boxplot(fill = "pink")+
  
  #Simplifying Theme
  theme_bw()+
  
  #Adding Useful Labes
  labs(title = "Relationship between weight and physical activity",
       subtitle = "People exercising less than 3 times a week seem to be lighter, therefore we suggest to avoid exercising",
       x = "Participant's Weight",
       y = "Do participants exercise ar least 3 times a week?"  
      ) +
  NULL


```


Looking at the box plot we believe that there is a relationship between the variables. However, the outcome is surprising because people exercising less than 3 times a week have a smaller average weight than people exercising at least 3 times. Before running the code, we were thinking the contrary would be true since people exercising a lot would be expected to be lighter.

## Confidence Interval

Boxplots show how the medians of the two distributions compare, but we can also compare the means of the distributions using either a confidence interval or a hypothesis test.

We start by computing the confidence intervals below to see this. 


```{r, ci_using_formulas}

#Setting up a table with a Yes/No for 3 days active or more
physical_3plus<-yrbss %>%
  mutate(physical_3plus=ifelse(physically_active_7d>=3, "yes","no"))


#Using above dataset for the confidence interval calculations
formula_ci_yes <- physical_3plus %>% 
  
  #Filtering for the people who are active 3 adds or more
  filter(physical_3plus=="yes") %>% 
  
  #Calculate weight's summary statistics for people exercising at least 3 times a week 
  
  # calculate mean, SD, count, SE, lower/upper 95% CI
  summarise(
    average_weight=mean(weight,na.rm=TRUE), #Mean, we choose to ignore any missing values by setting the 'na.rm = TRUE'
            
    sd_weight=sd(weight,na.rm=TRUE), #Standard Deviation
            
    count= n(), #Observations
           
     t_critical = qt(0.975,count-1), #T-Critical at 95% Confidence Interval and these observations
            
    se_weight=sd_weight/sqrt(count), #Standard Error 
           
    margin_of_error= t_critical*se_weight, #Margin of Error
            
    weight_low= average_weight - margin_of_error, #Lower interval
            
    weight_high= average_weight + margin_of_error) #Upper Interval 

formula_ci_no <- physical_3plus %>% 
  
  #We now repeat the process for people who are inactive
  filter(physical_3plus=="no") %>% 
  
  #Calculate weight's summary statistics for people exercising less than 3 times a week
  
  #Comments are ommitted as they are the same as above
  summarise(
    average_weight=mean(weight,na.rm=TRUE),
            
    sd_weight=sd(weight,na.rm=TRUE),
            
    count= n(),
            
    t_critical = qt(0.975,count-1),
            
    se_weight=sd_weight/sqrt(count),
            
    margin_of_error= t_critical*se_weight,
            
    weight_low= average_weight - margin_of_error,
            
    weight_high= average_weight + margin_of_error)   # calculate mean, SD, count, SE, lower/upper 95% CI

stats_yes_no <- bind_rows(formula_ci_yes,formula_ci_no)

stats_yes_no

```

There is an observed difference of about 1.77kg (68.44 - 66.67), and we notice that the two confidence intervals do not overlap. It seems that the difference is at least 95% statistically significant. This is interesting because people who exercise more are heigher in weight. 

This could either be beacuse 1: Those people have more muscle, or 2: People at higher weights are more motivated to exercise. 

Let us also conduct a hypothesis test to confirm our answers.

## Hypothesis test with formula

Below we write out the hypothesis and test this in a t.test using R's functions. 

```{r, t_test_using_R}


#Null Hypothesis is that the means are the same

#Alternative hypothesis is that the means are different 

#We can use the below formula to test the sample
t.test(weight ~ physical_3plus, data = physical_3plus)


```

We get a very low p-value and we can reject the null hypothesis. I.e. the means are different. 

## Hypothesis test with `infer`

We will now see how we can do this using the infer package. 

We need to initialize the test, which we will save as `obs_diff`.

This is done below

```{r, calc_obs_difference}

#New Test to be initialized
obs_diff <- physical_3plus %>%
  
  #Variable weight with Yes/No Input
  specify(weight ~ physical_3plus) %>%
  
  #We want to look at difference in means
  calculate(stat = "diff in means", order = c("yes", "no"))

```

To test whether there is a difference in means we will use yes - no != 0. This means Yes mean minus no mean is not equal to zero.

We now need to simulate the test on the null distribution, which we will save as null.

This is completed below. 

```{r, hypothesis_testing_using_infer_package}

null_dist <- physical_3plus %>%
  # specify variables
  specify(weight ~ physical_3plus) %>%
  
  # assume independence, i.e, there is no difference
  hypothesize(null = "independence") %>%
  
  # generate 1000 reps, of type "permute", which is the argument when generating a null distribution for a hypothesis test
  generate(reps = 1000, type = "permute") %>% 
  
  # calculate statistic of difference, namely "diff in means"
  calculate(stat = "diff in means", order = c("yes", "no"))

```


Here, `hypothesize` is used to set the null hypothesis as a test for independence, i.e., that there is no difference between the two population means. In one sample cases, the null argument can be set to *point* to test a hypothesis relative to a point estimate.

We can visualize this null distribution with the following code:

```{r}

#Null hypothesis
ggplot(data = null_dist, aes(x = stat)) +
  
  #Tested with Historgram
  geom_histogram()+
  
  #Simple Theme
  theme_bw()

```


With the test initialized and the null distribution formed, we can visualise to see how many of these null permutations have a difference of at least `obs_stat` of `r obs_diff %>% pull() %>% round(2)`:

We can also calculate the p-value for your hypothesis test using the function `infer::get_p_value()`.

```{r}

#Visualizing the data
null_dist %>% visualize() +
  
  #Two sided test
  shade_p_value(obs_stat = obs_diff, direction = "two-sided")+
  theme_bw()

# calculating p value for two tails testing
null_dist %>%
  get_p_value(obs_stat = obs_diff, direction = "two_sided")

```


This is useful and shows us that only very few observations if any have a difference of 1.77 kg. I.e. It shows us that the means are significantly different. 

# IMDB ratings: Differences between directors

Now, we would like to explore whether the mean IMDB rating for Steven Spielberg and Tim Burton are the same or not. Thanks to the confidence intervals already being calculated for the mean ratings of these two directors, we can see they overlap.

Our next objective will be to reproduce the graph above.

We will also run a hypothesis test to test the difference between the two means. 

We load the dataframe with the below code

```{r load-movies-data}
movies <- read_csv(here::here("data", "movies.csv"))

#We want to get an overview of dataframe structure
glimpse(movies)
```

We conduct the analysis and reproduce the graph in the following way:

```{r calculating confidence intervals and ploting the results}

#Our null Hypothesis is that they are the same mean ratings

#Our Alternative hypothesis is that they have different mean ratings


#Filtering data related to Steven Spielberg and Tim Burton
Spielberg_Burton <- movies %>%
  filter(director %in% c("Tim Burton","Steven Spielberg"))

#Running t-test
t.test(rating ~ director, data = Spielberg_Burton)

#Already here we get a p-value of 0.01 showing there is a difference in ratings between the two. 

#Using infer package to simulate from a null distribution

#WE form a null hypothesis
ratings_null <- Spielberg_Burton %>%
  
  #With rating by director 
  specify(rating ~ director) %>%
  
  #Hypothesize a null of no (or zero) difference
  hypothesize(null = "independence") %>%
  
  #Repetitions
  generate(reps = 1000, type = "permute") %>%
  
  #Looking at difference in means
  calculate(stat = "diff in means", order = c("Tim Burton","Steven Spielberg"))


#We now visualize this
ratings_null %>% visualize()

#With the visualisation, we can see that the null hypothesis seems to follow a normal distribution. 
#Data points are equally spread around the mean. 
#Furthermore, our p-value is less than 5%, meaning it is statistically significant. 
#Based on this value, we think we can reject the null hypothesis that there is no difference between the two variables. 
#However, we still need to see whether confidence intervals overlap to determine if the alternative hypothesis is true.

#Calculate summary statistics to find low and high IMDB
Spielberg_Burton_low_high <- movies %>%
  
  #Group by director
  group_by(director) %>%
  
  #Using Spielberg and Burton
  filter(director %in% c("Tim Burton","Steven Spielberg")) %>%
  
  #Summarizing statistics (comments ommitted - they are included in a previus input for these calculations)
  summarise(Mean_IMDB =mean(rating),
            sd_IMDB=sd(rating),
            count= n(),
            t_critical = qt(0.975,count-1),
            se_IMDB=sd_IMDB/sqrt(count),
            margin_of_error= t_critical*se_IMDB,
            IMDB_low= Mean_IMDB - margin_of_error,
            IMDB_high= Mean_IMDB + margin_of_error)   # calculate mean, SD, count, SE, lower/upper 95% CI


#Factor for the order
Spielberg_Burton_low_high$director<-factor(Spielberg_Burton_low_high$director,levels=c("Tim Burton","Steven Spielberg"))

#We can now see a table of our summary statistics
Spielberg_Burton_low_high

#We already see the confidence intervals overlap

#Now we will plot our results to obtain the same graph as previously seen:
ggplot(Spielberg_Burton_low_high, aes(x=Mean_IMDB, y=director, color=director)) +
  
  #geom_errorbar function allows us to show the two bars with confidence intervals
  geom_errorbar(aes(xmin=IMDB_low, xmax=IMDB_high),width = 0.1, size=3) +
  
  #geom_rect function then allows us to highlight where the two confidence intervals overlap
  geom_rect(data=Spielberg_Burton_low_high, aes(xmin=IMDB_low[1],xmax=IMDB_high[2]),ymin=-Inf, ymax=Inf, alpha=0.4,fill="grey",color="grey")+
  
  #Adding points for the inputs
  geom_point(size = 7) +
  
  #With the geom_text_repel function, we choose where to position the different labels
  ggrepel::geom_text_repel(aes(label=round(Mean_IMDB,2),nudge_y=c(2.1,1.1)),color="black",size=6.5,segment.alpha=0)+
  ggrepel::geom_text_repel(aes(label=round(IMDB_low,2),nudge_x=IMDB_low,nudge_y=c(2.1,1.1)),color="black",size=5,segment.alpha=0)+
  ggrepel::geom_text_repel(aes(label=round(IMDB_high,2),nudge_x=IMDB_high,nudge_y=c(2.1,1.1)),color="black",size=5,segment.alpha=0)+
  
  #With a nicer black and white theme
  theme_bw() +
  
  #Finally, we adapt the legend and titles to approach the original graph
  theme(legend.position = "none",axis.title.y=element_blank())+
  labs(title = "Do Spielberg and Burton have the same mean IMDB ratings?",
       subtitle = "95% confidence intervals overlap",
       x = "Mean IMDB Rating"
       ) +
  NULL



```

We see from the chart that the confidence intervals overlap for Steven Spielberg and Tim Burton however, our p-value is still 0.01 indicating that they are different within a 95% confidence interval. 

STated otherwise we reject our null hypothesis that they have the same mean rating. 


# Omega Group plc- Pay Discrimination

We now take a look at pay discrimination in Omega Group PLC. 

We, of course, take a stastical approach to look at this to avoid biases. 

An  issue was raised that women were being discriminated in the company, in the sense that the salaries were not the same for male and female executives. A quick analysis of a sample of 50 employees (of which 24 men and 26 women) revealed that the average salary for men was about 8,700 higher than for women.

Therefore, this analysis will look into that difference. 

We want to find whether there is indeed a significant difference between the salaries of men and women, and whether the difference is due to discrimination or whether it is based on another, possibly valid, determining factor. 

## Loading the data


```{r load_omega_data}
omega <- read_csv(here::here("data", "omega.csv"))

glimpse(omega) # examine the data frame
```

## Relationship Salary - Gender ?

Our dataframe shows the salary, gender, and an experience variable between the employees. 

We can use the experience variable to control for whether this pay gap is based on gender or other factors. 

We can perform different types of analyses, and check whether they all lead to the same conclusion 

.	Confidence intervals
.	Hypothesis testing
.	Correlation analysis
.	Regression


First we We will calculate summary statistics on salary by gender and show summary statistics. 

See below: 

```{r, confint_single_valiables}

# Summary Statistics of salary by gender
mosaic::favstats (salary ~ gender, data=omega)

#From the chart we see that the median and mean pay is indeed higher for males than females.


#Lets now compute the summary statistics.
gender<-favstats (salary ~ gender, data=omega) %>% 
  
  #Grouping by gender
  group_by(gender) %>% 
  
  #Calculating Summary statistics
  summarize(mean=mean, #Mean calc.
            
            SD=sd, #Standard Deviation
            
            sample_size=n, #Sample Size
            
            t_critical=qt(0.975,sample_size-1), # 95% confidence interval t-stat

            se=SD/sqrt(sample_size), #Standard error

            margin_error=t_critical*se, # Calculate the margin error for the salary by gender

            salary_low=mean-margin_error, # Get the lower limits of the interval

            salary_high=mean+margin_error) # Get the upper limits of the interval

#Print results
gender

```

> From our analysis, we can see that the 95% confidence intervals of the two means do not overlap. It means that the difference between the two means is statistically significant. Therefore, the difference between average salary for men and women is statistically significant and we can assume there is a real difference to be analised. But it would still need to be confirmed by running a t-test. However, we cannot conclude that this is because of gender only from these statistics. 



We can also run a hypothesis testing, assuming as a null hypothesis that the mean difference in salaries is zero. See below a t.test and infer package results:

```{r, hypothesis_testing}

# hypothesis testing using t.test() 
t.test(salary ~ gender, data=omega)

#WE see we get a very low p-value i.e. the means are indeed different

# hypothesis testing using infer package
salary_null <- omega %>%
  
  #Specify the variable of interest
  specify(salary ~ gender) %>%
  
  #Hypothesize a null of no (or zero) difference
  hypothesize(null = "independence") %>%
  
  #Generate a bunch of simulated samples
  generate(reps = 1000, type = "permute") %>%
  
  #Find the mean difference of each sample
  calculate(stat = "diff in means", order = c("male","female"))


#Calculate the difference in means
obs_diff1 <- omega %>%
  
  #Salary based on gender
  specify(salary ~ gender) %>%
  
  #We want to look at difference in means
  calculate(stat = "diff in means", order = c("male", "female"))


#Print a historgram
salary_null %>% visualize() + 
    
  #Two sided test with our observations showing.
  shade_p_value(obs_stat = obs_diff1, direction = "two-sided")+
  theme_bw()
  

```

> Based on our analysis, the null hypothesis that there is no significant difference between average salary for men and women is rejected because the p-value is less than the 0.05 level.


## Relationship Experience - Gender?


We now want to confirm whether this is actually based on gender or actual experience. 

The experience of the worksers can be seen below: 

```{r, experience_stats}
# Summary Statistics of salary by gender
favstats (experience ~ gender, data=omega)

```

We immidieatly see that there is much higher mean and median experience in men. We want to look into this going forward. 

## Relationship Salary - Experience ?


Lets first plot how experience compares to salary

We will calculate summary statistics on experience by gender. We will also create and print a dataframe where, for each gender, we show the mean, SD, sample size, the t-critical, the SE, the margin of error, and the low/high endpoints of a 95% confidence interval


```{r, salary_exp_scatter}

#First lets calculate summary statistics
gender<-favstats (experience ~ gender, data=omega) %>% 
  
  #Grouping by gender
  group_by(gender) %>% 
  
  #And calculating summary statistics
  summarize(mean_experience=mean,
            SD=sd,
            sample_size=n,
            t_critical=qt(0.975,sample_size-1),
            se=SD/sqrt(sample_size),
            margin_error=t_critical*se,
            experience_low=mean-margin_error,
            experience_high=mean+margin_error)

#Printing the table.
gender

#We see that the Confidence interval does not overlap in experience. 

#We will also plot salary~experience
ggplot(omega,aes(x=experience, y=salary)) +
  
  #Using Geom Point
  geom_point(aes(colour=gender, alpha = 0.5))+ #colour by gender to see the differences
  
  #Simplifying the theme
  theme_bw()+
  
  #Removing Legend
  theme(legend.position="none")+
  
  #Adding a  trendline 
  geom_smooth(colour="black", alpha=0)+ #the correlation seems logarithmic 
  
  #Adding useful labels
  labs (
    title = "Correlation between Salary and Experience",
    x     = "Experience",
    y = "Salary" #changing y-axis label to sentence case, 
  )

```

We clearly see that salary increases with experience! Now this is interesting. Lets dive deeper into the issue. 


## Check correlations between the data

We compute a graph in the following way:

```{r, ggpairs}

omega %>% 
  select(gender, experience, salary) %>% #order variables they will appear in ggpairs()
  ggpairs(aes(colour=gender, alpha = 0.3))+
  theme_bw()

```

> What we can infer from this plot is that the relationship between salary and experience seems to follow an approximately linear progression. As we can from the graphs, salary and experience are more postively related for female than male. Especially for women with short experience, there seems to be a strong of disparity among average salaries. Furthermore, as people get more experience, men usually have the highest salary for the same years of experience.


# Challenge 1: Yield Curve inversion

Every so often, we hear warnings from commentators on the "inverted yield curve" and its predictive power with respect to recessions. An explainer what a [inverted yield curve is can be found here](https://www.reuters.com/article/us-usa-economy-yieldcurve-explainer/explainer-what-is-an-inverted-yield-curve-idUSKBN1O50GA).

In addition, many articles and commentators think that, e.g., [*Yield curve inversion is viewed as a harbinger of recession*](https://www.bloomberg.com/news/articles/2019-08-14/u-k-yield-curve-inverts-for-first-time-since-financial-crisis). One can always doubt whether inversions are truly a harbinger of recessions, and [use the attached parable on yield curve inversions](https://twitter.com/5_min_macro/status/1161627360946511873).



We are gonna use the [FRED database](https://fred.stlouisfed.org/) to download historical yield curve rates, and plot the yield curves since 1999 to see when the yield curves flatten. 

First, we will load the yield curve data file that contains data on the yield curve since 1960-01-01

```{r download_historical_yield_curve, warning=FALSE}

yield_curve <- read_csv(here::here("data", "yield_curve.csv"))

glimpse(yield_curve)
```
We see that we get the following variables 

- `date`: already a date object
- `series_id`: the FRED database ticker symbol
- `value`: the actual yield on that date
- `maturity`: a short hand for the maturity of the bond
- `duration`: the duration, written out in all its glory!

These will form the base of our plot and analysis

## Plotting the yield curve

Let us compute the following 3 graphs presenting the data:

### Yields on US rates by duration since 1960


```{r,fig,width=50}

#First setup a factor for the order of duration we want to plot
yield_curve1 <- yield_curve %>% 
  
  #We setup our preferred order
  mutate(duration_1 = factor(yield_curve$duration, levels=c("3-Month Treasury Bill",
                                                            "6-Month Treasury Bill",
                                                            "1-Year Treasury Rate",
                                                            "2-Year Treasury Rate",
                                                            "3-Year Treasury Rate",
                                                            "5-Year Treasury Rate",
                                                            "7-Year Treasury Rate",
                                                            "10-Year Treasury Rate",
                                                            "20-Year Treasury Rate",
                                                            "30-Year Treasury Rate")))
  
#plot the data with dates on x axis and value on y axis
ggplot(yield_curve1,aes(x=date,y=value,color=duration))+
  
  #Using a line plot
  geom_line()+
  
  #Splitting by duration
  facet_wrap(~duration_1,ncol=2)+
  
  #Simple theme
  theme_bw()+
  
  #Removing legend
  theme(legend.position = "none")+
  
  #Adding useful labels. 
  labs(title="Yields on U.S. Treasury rates since 1960",
       y="%",
       x=NULL,
       caption = "Source: St.Louis Federal Reserve Economic Database(FRED)")+
  NULL


```

Interestingly, we see that the rates to indeed swing somewhat in paralel overtime. 

### Monthly yields on US rates by duration since 1999 on a year-by-year basis

We reproduce that plot by utitilizing the code below:

```{r,fig.width=10}

yield_curve2 <- yield_curve %>% 
  
  #subtract the month and year from the dataset
  mutate(year = year(date),
         month=month(date)) %>% 
  
  #select data of year after 1999
  filter(year >= 1999)

#group by each month and draw the line
ggplot(yield_curve2,aes(x=as.factor(maturity),y=value,group=month,color=factor(year)))+
  geom_line()+
  
  #facet by year
  facet_wrap(~year,ncol=4)+
  
  #manually give value to x
  scale_x_discrete(limits=c("3m","6m","1y","2y","3y","5y","7y","10y","20y","30y"))+
  
  #Simple theme
  theme_bw()+
  
  #Removing legend
  theme(legend.position = "none")+
  
  #Adding useful labels
  labs(title="US Yield Curve",
       y="Yield(%)",
       x="Maturity",
       caption = "Source: St.Louis Federal Reserve Economic Database(FRED)")+
  NULL


```


What we see is that over the years the yield has been slowly falling across all durations.

### 3-month and 10-year yields since 1999

We reproduce that plot by utitilizing the code below:

```{r,fig.width=10}

#select data of year after 1999 & 3-Month Treasury Bill & 10-Year Treasury Rate, calculate the average rate
yield_curve3 <- yield_curve %>% 
  
  #Getting the year
  mutate(year=year(date)) %>% 
  
  #Filtering by years above 1999
  filter(year >= 1999) %>% 
  
  #Filtering the durations we need
  filter(duration == "3-Month Treasury Bill" | duration=="10-Year Treasury Rate") %>% 
  
  #Grouping by duration and date
  group_by(duration,date) %>% 
  
  #Getting the mean rates 
  mutate(yield_avg=mean(value))
  
  

#We now plot this
ggplot(yield_curve3,aes(x=date,y=yield_avg,color=duration))+
  
  #Line plot
  geom_line()+
  
  #Adding a scale for the year
  scale_x_date(breaks=seq(as.Date("2000-01-01"),as.Date("2020-01-01"),by="5 years"),date_labels = "%Y")+
  
  #Simple theme
  theme_bw()+
  
  #White legend
  theme(legend.title=element_blank())+
  
  #Useful labels
  labs(title = "Yield Curve Inversion: 10-year minus 3-month U.S. Treasury rates", 
       subtitle = "Difference in % points, monthly averages.
       Shaded area correspond to recessions",
       y = "Difference (10 year-3 month) yield in %",
       caption = "Source: FRED, Federal Reserve Bank of St. Louis") + 
  NULL


```

>A flattening yield curve is defined as the narrowing of the yield spread between long and short team interest rates. In this case, we can compare the yields of 3-month treasury bill and 10-year treasury rate to confirm whether the yield curve seem to flatten before Mar 2001-Nov 2001 and Dec 2007-June 2009 recessions in the US. Based on the graph we reproduced above, we can see that the yield curve flatten before Mar 2001 and Dec 2007 and recessions followed shortly after. With limited dataset, we cannot be 100% confident that a yield curve flattening really mean a recession is coming in the US. However, there is a high possibility that those two incidents are highly related. Since 1999, short-term yield was larger than longer-term yield in 2001, 2007 and 2019. Since recessions followed when short-term yield was larger than longer-term yield in 2001 and 2007, we need to pay a closer attention to the financial market to see whether the next recession will follow after 2019 yield curve flattening.  

### The Final Graph of Yield Curve


Besides calculating the spread (10year - 3months), we need to first set up data for US recessions.
The code below creates a dataframe with all US recessions since 1946: 

```{r setup_US-recessions, warning=FALSE}

# get US recession dates after 1946 from Wikipedia 
# https://en.wikipedia.org/wiki/List_of_recessions_in_the_United_States

recessions <- tibble(
  from = c("1948-11-01", 
           "1953-07-01", 
           "1957-08-01", 
           "1960-04-01", 
           "1969-12-01", 
           "1973-11-01", 
           "1980-01-01",
           "1981-07-01", 
           "1990-07-01", 
           "2001-03-01", 
           "2007-12-01",
           "2020-02-01"),  
  
  to = c("1949-10-01", 
         "1954-05-01", 
         "1958-04-01", 
         "1961-02-01", 
         "1970-11-01", 
         "1975-03-01", 
         "1980-07-01", 
         "1982-11-01", 
         "1991-03-01", 
         "2001-11-01", 
         "2009-06-01", 
         "2020-04-30") 
  )  %>% 
  
  mutate(From = ymd(from), 
         To=ymd(to),
         duration_days = To-From)


recessions
```

Then we reproduce that plot by utitilizing the code below:

```{r,fig.width=20}

#produce a new dataset with 3-Month & 10-Year Treasury Rate
yield_curve_long <- yield_curve %>% 
  
  #Getting the year
  mutate(year=year(date)) %>% 
  
  #Filtering by our required durations
  filter(duration == "3-Month Treasury Bill" | duration=="10-Year Treasury Rate") %>% 
  
  #Grouping by date and maturity
  group_by(duration,date) %>% 
  
  #Getting mean yield
  mutate(yield_avg=mean(value))

#select the 3-Month treasury bill
yield_curve4 <- yield_curve_long %>% 
  select(maturity,date,yield_avg) %>% 
  filter(maturity=="3m")

#select the 10-year treasury rate
yield_curve5 <- yield_curve_long %>% 
  select(maturity,date,yield_avg) %>% 
  filter(maturity == "10y")

#combine the 3-month& 10-year treasury rate
yield_curve_rece <- merge(yield_curve4,yield_curve5,by="date") %>% 
  
  #Setting date format
  mutate(Date = as.Date(date,format = "%Y-%M-%D"),
         
         minus_treasury_rate=yield_avg.y-yield_avg.x, #calculate the 10-year rate minus the 3-month rate
         
         color_id = ifelse(minus_treasury_rate<0, 1, 0)) %>%  #define the color for filling 
  
  select(Date,minus_treasury_rate,color_id) #Selecting relevant columns

#define the function for y-ray
scaleFUN <- function(x) sprintf("%.1f", x)


#filter the date of recessions
recessions_1 <- recessions %>% 
  filter(From >= as.Date("1960-01-01") | To >= as.Date("1960-01-01"))

#produce the graph
ggplot(yield_curve_rece, aes(x = Date , y=minus_treasury_rate)) + 
  geom_line(color = "black", size = 0.2) + 
  
  #Ribbons which shade in blue and red
  geom_ribbon(aes(ymin=0,ymax=ifelse(minus_treasury_rate>0, minus_treasury_rate, 0)),fill="skyblue",alpha=0.4)+
  geom_ribbon(aes(ymax=0,ymin=ifelse(minus_treasury_rate<0, minus_treasury_rate, 0)),fill="salmon",alpha=0.4) +
  
  #Rug which colors based on a the variable computed above and on the x axis
  geom_rug(mapping = aes(color = factor(color_id)), sides = "b") +
  
  #Setting the rug to blue and red
  scale_color_manual(values = c("skyblue", "salmon")) +
  
  #Simple theme
  theme_bw() + 
  
  #add a line for y=0
  geom_hline(aes(yintercept=0))+
  
  #Removing the legend
  guides(color=FALSE) + 
  
  #Scaling the axis and setting year numbers for the x axis
  scale_x_date(breaks = seq(as.Date("1959-01-01"),as.Date("2023-01-01"),by="2 years"), date_labels = "%Y") + 
  
  #Setting y axis
  scale_y_continuous(labels=scaleFUN)+
  
  #Adding shading by recession data
  geom_rect(data= recessions_1, inherit.aes = FALSE,aes(xmin = From, xmax = To, ymin =-Inf , ymax = Inf), fill = "grey", alpha = 0.3)+

  #Simplifying the theme
  theme(panel.border=element_blank(),
        panel.background = element_blank(),
        strip.background = element_rect(color="white",fill="white"), 
        axis.title.y = element_blank()) + 
  
  #Useful labels
  labs(title = "Yield Curve Inversion: 10-year minus 3-month U.S. Treasury rates", 
       subtitle = "CDifference in % point, monthly averages. \n Shaded areas correspond to recessions ",
       x = NULL,
       y = "Difference (10 year-3 month) yield in %",
       caption="Source: FRED, Federal Reserve Bank of St. Louis")+
  NULL 



```


``